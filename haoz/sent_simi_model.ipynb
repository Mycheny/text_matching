{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:55:47.782842Z",
     "start_time": "2020-04-07T23:55:42.678489Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "from wordfreq import word_frequency\n",
    "import jieba\n",
    "\n",
    "import time\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:55:47.796805Z",
     "start_time": "2020-04-07T23:55:47.785834Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def init_dict(dictfile='tc_min.dict'):\n",
    "    # jieba 加载自定义词典\n",
    "    jieba.load_userdict(dictfile)\n",
    "    jieba.enable_paddle()\n",
    "    \n",
    "    # 加载词频数据并返回\n",
    "    domain = int(2 ** 31 - 1)\n",
    "    freq_dict = {}\n",
    "    with open(dictfile, 'r',encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            segs = line.split(' ')\n",
    "            token = segs[0]\n",
    "            freq = int(segs[1])\n",
    "        freq_dict[token] = float(freq / domain)\n",
    "    \n",
    "    return freq_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:57:21.652875Z",
     "start_time": "2020-04-07T23:55:47.804787Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Xiaoi\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.822 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tc_min.dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-87e10eb566e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfreq_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-f2ee95ae3e64>\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(dictfile)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tc_min.dict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# jieba 加载自定义词典\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_userdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_paddle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xiaoi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\jieba\\__init__.py\u001b[0m in \u001b[0;36mload_userdict\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m             \u001b[0mf_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m             \u001b[0mf_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresolve_filename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tc_min.dict'"
     ]
    }
   ],
   "source": [
    "freq_dict = init_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:57:21.661847Z",
     "start_time": "2020-04-07T23:57:21.655862Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_word_frequency(word,freq_dict=freq_dict):\n",
    "    \n",
    "    if word in freq_dict:\n",
    "        return freq_dict[word]\n",
    "    else:\n",
    "        return word_frequency(word, 'zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:57:21.689777Z",
     "start_time": "2020-04-07T23:57:21.664839Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "\n",
    "def init_index(annoy_indexfile = 'tc_index_build10.ann.index',word2indexfile='tc_word_index.json'):\n",
    "    # 我们用保存好的索引文件重新创建一个Annoy索引, 单独进行加载\n",
    "    annoy_index = AnnoyIndex(200)\n",
    "    annoy_index.load(annoy_indexfile)\n",
    "    \n",
    "    with open(word2indexfile, 'r') as fp:\n",
    "        word_index = json.load(fp)\n",
    "\n",
    "    #准备一个反向id==>word映射词表\n",
    "    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "    \n",
    "    return annoy_index,word_index,reverse_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:58:30.810498Z",
     "start_time": "2020-04-07T23:57:21.692766Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "annoy_index,word_index,reverse_word_index = init_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:58:30.871336Z",
     "start_time": "2020-04-07T23:58:30.829449Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def AVG_embedding(line, embed_index=annoy_index,word2index=word_index,dim=200,pc=0):\n",
    "    #start = time.time()\n",
    "    \n",
    "    word_list = [token for token in list(jieba.cut(line,use_paddle=True))\n",
    "                 if token in word2index.keys()]\n",
    "    \n",
    "    #stop = time.time()\n",
    "\n",
    "    #print(\"time for cut words = %.2f s\" % (float(stop - start)))\n",
    "\n",
    "\n",
    "    #start = time.time()\n",
    "    sent_length = len(word_list)\n",
    "    vs = np.zeros(dim)\n",
    "    if not sent_length:\n",
    "        return vs\n",
    "    for token in word_list:\n",
    "        vs += embed_index.get_item_vector(word2index[token])\n",
    "    \n",
    "    #stop = time.time()\n",
    "    #print(\"time for calc avg vector = %.2f s\" % (float(stop - start)))\n",
    "    \n",
    "    return vs / sent_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:58:30.986031Z",
     "start_time": "2020-04-07T23:58:30.875326Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "AVG_embedding(\"喜欢打篮球的男生喜欢什么样的女生\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:58:31.005976Z",
     "start_time": "2020-04-07T23:58:30.991016Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "\n",
    "def FREQ_embedding(line, embed_index=annoy_index,word2index=word_index,dim=200, a=1e-3,pc=0):\n",
    "    #start = time.time()\n",
    "    word_list = [token for token in list(jieba.cut(line,use_paddle=True))\n",
    "                 if token in word2index.keys()]\n",
    "    #stop = time.time()\n",
    "\n",
    "    #print(\"time for cut words = %.2f s\" % (float(stop - start)))\n",
    "    \n",
    "    sent_length = len(word_list)\n",
    "    vs = np.zeros(dim)\n",
    "    \n",
    "    #start = time.time()\n",
    "    \n",
    "    if not sent_length:\n",
    "        return vs\n",
    "    for token in word_list:\n",
    "        token_freq = get_word_frequency(token)         \n",
    "        a_value = a / (a + token_freq)        \n",
    "        vs += a_value * array(embed_index.get_item_vector(word2index[token]))\n",
    "    \n",
    "    #stop = time.time()\n",
    "    #print(\"time for calc weighted vector = %.2f s\" % (float(stop - start)))\n",
    "    \n",
    "    return vs / sent_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:58:31.388951Z",
     "start_time": "2020-04-07T23:58:31.008968Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "FREQ_embedding(\"无线上网卡和无线路由器怎么用\").shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:58:31.401920Z",
     "start_time": "2020-04-07T23:58:31.394945Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def SVD_Lines(lines):\n",
    "    start = time.time()\n",
    "    \n",
    "    matrix = np.array([FREQ_embedding(line)\n",
    "                       for line in lines])\n",
    "    \n",
    "    stop = time.time()\n",
    "\n",
    "    print(\"time for calc matrix = %.2f s\" % (float(stop - start)))\n",
    "    \n",
    "    start = time.time()\n",
    "    svd = TruncatedSVD(n_components=1, n_iter=7, random_state=0)\n",
    "    svd.fit(matrix)\n",
    "    pc = svd.components_\n",
    "    \n",
    "    stop = time.time()\n",
    "\n",
    "    print(\"time for calc pc = %.2f s\" % (float(stop - start)))\n",
    "    return pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:58:31.479708Z",
     "start_time": "2020-04-07T23:58:31.404909Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "lines = [\"新概念英语第二册练习册41课答案\",\"无线上网卡和无线路由器怎么用\",\"福州哪家装修公司好\"]\n",
    "pc = SVD_Lines(lines)\n",
    "print(pc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:58:31.490680Z",
     "start_time": "2020-04-07T23:58:31.485697Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def SIF_embedding(line,pc):\n",
    "    embed = FREQ_embedding(line)\n",
    "    return embed - embed @ pc.T @ pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:58:31.533565Z",
     "start_time": "2020-04-07T23:58:31.493677Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SIF_embedding(\"石家庄天气如何？\",pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:31:51.147674Z",
     "start_time": "2020-04-08T22:31:51.126725Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def load_sentence_data(file_path):\n",
    "    sentences1 = []\n",
    "    sentences2 = []\n",
    "    labels = []\n",
    "    with open(file_path, 'r',encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            s1, s2, label = line.split('\\t')\n",
    "            if not label:\n",
    "                continue\n",
    "            labels.append(int(label))\n",
    "            sentences1.append(s1)\n",
    "            sentences2.append(s2)\n",
    "    return sentences1, sentences2, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:58:31.559496Z",
     "start_time": "2020-04-07T23:58:31.549523Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "LCQMC = [\n",
    "    'LCQMC/processed/train.tsv',\n",
    "    'LCQMC/processed/dev.tsv',\n",
    "    'LCQMC/processed/test.tsv',\n",
    "    ]\n",
    "\n",
    "CCKS = [\n",
    "    'CCKS/processed/train.tsv',\n",
    "    'CCKS/processed/dev.tsv',\n",
    "    'CCKS/processed/test.tsv',\n",
    "    ]\n",
    "\n",
    "ATEC = [\n",
    "    'ATEC/processed/train.tsv',\n",
    "    'ATEC/processed/dev.tsv',\n",
    "    'ATEC/processed/test.tsv',\n",
    "    ]\n",
    "\n",
    "CORPUS = [\n",
    "    (LCQMC,'LCQMC'),\n",
    "    (CCKS, 'CCKS'),\n",
    "    (ATEC, 'ATEC')    \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:58:31.578446Z",
     "start_time": "2020-04-07T23:58:31.562488Z"
    },
    "code_folding": [],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def texts_to_embeddings(texts, embedding_method, pc):\n",
    "    embedding_list = []\n",
    "    for text in texts:\n",
    "        embedding = embedding_method(text,pc)\n",
    "        embedding_list.append(embedding)\n",
    "    return np.array(embedding_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:58:31.595402Z",
     "start_time": "2020-04-07T23:58:31.581438Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def build_dataset(dataset,embedding_method,pc):\n",
    "    s1,s2,labels = load_sentence_data(dataset)        \n",
    "    left_X = texts_to_embeddings(s1, embedding_method,pc)\n",
    "    right_X = texts_to_embeddings(s2, embedding_method,pc)\n",
    "    Y = np.array(labels)\n",
    "    \n",
    "    return [left_X, right_X], Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:58:31.609362Z",
     "start_time": "2020-04-07T23:58:31.598393Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def build_SVD(train_file,dev_file):\n",
    "    sentences = []\n",
    "\n",
    "    s1,s2,labels = load_sentence_data(train_file)\n",
    "\n",
    "    sentences.extend(s1)\n",
    "    sentences.extend(s2)\n",
    "\n",
    "    s1,s2,labels = load_sentence_data(dev_file)\n",
    "\n",
    "    sentences.extend(s1)\n",
    "    sentences.extend(s2)\n",
    "\n",
    "    pc = SVD_Lines(sentences)\n",
    "    \n",
    "    return pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:58:36.131931Z",
     "start_time": "2020-04-07T23:58:31.612354Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras import backend as K\n",
    "from keras.layers import Lambda, Dense\n",
    "from keras.layers import concatenate, multiply, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:58:36.144897Z",
     "start_time": "2020-04-07T23:58:36.133927Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def build_model(dim_size=200,dense_unit=100):\n",
    "    u_input = Input(shape=(dim_size,), dtype='float32', name=\"left_x\")\n",
    "    v_input = Input(shape=(dim_size,), dtype='float32', name='right_x')\n",
    "    \n",
    "    u_sub_v = Lambda(lambda x: K.abs(x[0] - x[1]))([u_input, v_input])\n",
    "    u_mul_v = multiply([u_input, v_input])\n",
    "    \n",
    "    u_concat_v = concatenate([u_input, v_input, u_sub_v, u_mul_v])\n",
    "    \n",
    "    dense = Dense(dense_unit, activation='relu')(u_concat_v)\n",
    "    similarity = Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = Model([u_input, v_input], similarity)\n",
    "    #print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:58:36.165862Z",
     "start_time": "2020-04-07T23:58:36.147894Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:58:36.184794Z",
     "start_time": "2020-04-07T23:58:36.172822Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_x, train_y, dev_x, dev_y,checkpointpath,lr=1e-3,batch_size=128,epochs=32):\n",
    "    adam = Adam(lr=lr)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                       optimizer=adam,\n",
    "                       metrics=['accuracy'])    \n",
    "    checkpoint_callback = ModelCheckpoint(checkpointpath,\n",
    "                                          monitor='val_acc',\n",
    "                                          verbose=0,\n",
    "                                          save_best_only=True,\n",
    "                                          save_weights_only=True,\n",
    "                                          mode='auto',\n",
    "                                          period=1)\n",
    "    model.fit(train_x, train_y,\n",
    "               batch_size=batch_size,\n",
    "               epochs=epochs,\n",
    "               validation_data=(dev_x, dev_y),\n",
    "               shuffle=True,\n",
    "               verbose=0,\n",
    "               callbacks=[checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:58:36.201745Z",
     "start_time": "2020-04-07T23:58:36.190775Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def model_predict(model, test_x, test_y=None,predict_batchsize=128):\n",
    "    \n",
    "    predict_y = model.predict(test_x, batch_size=predict_batchsize)[:, 0]\n",
    "    if test_y.any():\n",
    "        predict_y[predict_y >= 0.5] = 1\n",
    "        predict_y[predict_y < 0.5] = 0\n",
    "        print(classification_report(test_y, predict_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:58:36.217703Z",
     "start_time": "2020-04-07T23:58:36.204737Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def evaluate(dataset,embedding_method,checkpoint):    \n",
    "    dataset_dir = Path('../datasets/sentence-similarity-zoo-master/data')\n",
    "    train_file = dataset_dir / dataset[0]\n",
    "    dev_file = dataset_dir / dataset[1]\n",
    "    test_file = dataset_dir / dataset[2]\n",
    "    \n",
    "    pc = build_SVD(train_file,dev_file)\n",
    "    \n",
    "    train_x, train_y = build_dataset(train_file,SIF_embedding,pc)\n",
    "    dev_x, dev_y = build_dataset(dev_file,SIF_embedding,pc)\n",
    "    test_x, test_y = build_dataset(test_file,SIF_embedding,pc)\n",
    "    \n",
    "    model = build_model()\n",
    "    \n",
    "    train_model(model, train_x, train_y,dev_x, dev_y,checkpointpath=checkpoint )\n",
    "    \n",
    "    model.load_weights(checkpoint)\n",
    "    \n",
    "    model_predict(model,test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:31:51.113760Z",
     "start_time": "2020-04-07T23:58:36.221693Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for embedding_method, embedding_name in [\n",
    "    (AVG_embedding,'average weighted'),\n",
    "    (FREQ_embedding,'freq weighted'),\n",
    "    (SIF_embedding,'freq weighted + SVD')\n",
    "    ]:\n",
    "    for data in CORPUS:\n",
    "        print('------Embedding Method:{0}, DataSet:{1}---------'.format(embedding_name,data[1]))\n",
    "        checkpoint = embedding_name + \"_\" + data[1] + \"_best.h5\"\n",
    "        evaluate(data[0], embedding_method,checkpoint)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------Embedding Method:average weighted, DataSet:LCQMC---------\n",
    "time for calc matrix = 200.30 s\n",
    "time for calc pc = 8.87 s\n",
    "\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.80      0.70      0.74      6250\n",
    "           1       0.73      0.83      0.78      6250\n",
    "\n",
    "   micro avg       0.76      0.76      0.76     12500\n",
    "   macro avg       0.77      0.76      0.76     12500\n",
    "weighted avg       0.77      0.76      0.76     12500\n",
    "\n",
    "------Embedding Method:average weighted, DataSet:CCKS---------\n",
    "time for calc matrix = 95.08 s\n",
    "time for calc pc = 3.21 s\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.84      0.78      0.81      5001\n",
    "           1       0.79      0.85      0.82      4999\n",
    "\n",
    "   micro avg       0.82      0.82      0.82     10000\n",
    "   macro avg       0.82      0.82      0.81     10000\n",
    "weighted avg       0.82      0.82      0.81     10000\n",
    "\n",
    "------Embedding Method:average weighted, DataSet:ATEC---------\n",
    "time for calc matrix = 110.12 s\n",
    "time for calc pc = 3.74 s\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      0.94      0.93     13675\n",
    "           1       0.69      0.60      0.64      2886\n",
    "\n",
    "   micro avg       0.88      0.88      0.88     16561\n",
    "   macro avg       0.80      0.77      0.79     16561\n",
    "weighted avg       0.88      0.88      0.88     16561\n",
    "\n",
    "------Embedding Method:freq weighted, DataSet:LCQMC---------\n",
    "time for calc matrix = 205.51 s\n",
    "time for calc pc = 8.45 s\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.67      0.79      0.73      6250\n",
    "           1       0.75      0.61      0.67      6250\n",
    "\n",
    "   micro avg       0.70      0.70      0.70     12500\n",
    "   macro avg       0.71      0.70      0.70     12500\n",
    "weighted avg       0.71      0.70      0.70     12500\n",
    "\n",
    "------Embedding Method:freq weighted, DataSet:CCKS---------\n",
    "time for calc matrix = 81.72 s\n",
    "time for calc pc = 2.17 s\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.80      0.83      0.81      5001\n",
    "           1       0.82      0.79      0.81      4999\n",
    "\n",
    "   micro avg       0.81      0.81      0.81     10000\n",
    "   macro avg       0.81      0.81      0.81     10000\n",
    "weighted avg       0.81      0.81      0.81     10000\n",
    "\n",
    "------Embedding Method:freq weighted, DataSet:ATEC---------\n",
    "time for calc matrix = 171.28 s\n",
    "time for calc pc = 4.69 s\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.91      0.95      0.93     13675\n",
    "           1       0.72      0.57      0.64      2886\n",
    "\n",
    "   micro avg       0.89      0.89      0.89     16561\n",
    "   macro avg       0.82      0.76      0.79     16561\n",
    "weighted avg       0.88      0.89      0.88     16561\n",
    "\n",
    "------Embedding Method:freq weighted + SVD, DataSet:LCQMC---------\n",
    "time for calc matrix = 195.74 s\n",
    "time for calc pc = 5.52 s\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.82      0.63      0.71      6250\n",
    "           1       0.70      0.86      0.77      6250\n",
    "\n",
    "   micro avg       0.75      0.75      0.75     12500\n",
    "   macro avg       0.76      0.75      0.74     12500\n",
    "weighted avg       0.76      0.75      0.74     12500\n",
    "\n",
    "------Embedding Method:freq weighted + SVD, DataSet:CCKS---------\n",
    "time for calc matrix = 92.64 s\n",
    "time for calc pc = 4.27 s\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.81      0.82      0.82      5001\n",
    "           1       0.82      0.81      0.81      4999\n",
    "\n",
    "   micro avg       0.82      0.82      0.82     10000\n",
    "   macro avg       0.82      0.82      0.82     10000\n",
    "weighted avg       0.82      0.82      0.82     10000\n",
    "\n",
    "------Embedding Method:freq weighted + SVD, DataSet:ATEC---------\n",
    "time for calc matrix = 104.21 s\n",
    "time for calc pc = 6.26 s\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.91      0.95      0.93     13675\n",
    "           1       0.71      0.56      0.63      2886\n",
    "\n",
    "   micro avg       0.88      0.88      0.88     16561\n",
    "   macro avg       0.81      0.76      0.78     16561\n",
    "weighted avg       0.88      0.88      0.88     16561"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------Embedding Method:average weighted, DataSet:LCQMC---------\n",
    "time for calc matrix = 29267.18 s\n",
    "time for calc pc = 5.09 s\n",
    "\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.84      0.67      0.75      6250\n",
    "           1       0.73      0.88      0.79      6250\n",
    "\n",
    "   micro avg       0.77      0.77      0.77     12500\n",
    "   macro avg       0.79      0.77      0.77     12500\n",
    "weighted avg       0.79      0.77      0.77     12500\n",
    "\n",
    "------Embedding Method:average weighted, DataSet:CCKS---------\n",
    "time for calc matrix = 1448.29 s\n",
    "time for calc pc = 2.63 s\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.81      0.84      0.82      5001\n",
    "           1       0.83      0.80      0.82      4999\n",
    "\n",
    "   micro avg       0.82      0.82      0.82     10000\n",
    "   macro avg       0.82      0.82      0.82     10000\n",
    "weighted avg       0.82      0.82      0.82     10000\n",
    "\n",
    "------Embedding Method:average weighted, DataSet:ATEC---------\n",
    "time for calc matrix = 2378.26 s\n",
    "time for calc pc = 3.51 s\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.90      0.94      0.92     13675\n",
    "           1       0.65      0.50      0.57      2886\n",
    "\n",
    "   micro avg       0.87      0.87      0.87     16561\n",
    "   macro avg       0.77      0.72      0.74     16561\n",
    "weighted avg       0.86      0.87      0.86     16561\n",
    "\n",
    "------Embedding Method:freq weighted, DataSet:LCQMC---------\n",
    "time for calc matrix = 3045.52 s\n",
    "time for calc pc = 5.61 s\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.86      0.65      0.74      6250\n",
    "           1       0.72      0.89      0.80      6250\n",
    "\n",
    "   micro avg       0.77      0.77      0.77     12500\n",
    "   macro avg       0.79      0.77      0.77     12500\n",
    "weighted avg       0.79      0.77      0.77     12500\n",
    "\n",
    "------Embedding Method:freq weighted, DataSet:CCKS---------\n",
    "time for calc matrix = 1255.87 s\n",
    "time for calc pc = 2.08 s\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.83      0.81      0.82      5001\n",
    "           1       0.82      0.84      0.83      4999\n",
    "\n",
    "   micro avg       0.82      0.82      0.82     10000\n",
    "   macro avg       0.82      0.82      0.82     10000\n",
    "weighted avg       0.82      0.82      0.82     10000\n",
    "\n",
    "------Embedding Method:freq weighted, DataSet:ATEC---------\n",
    "time for calc matrix = 2198.74 s\n",
    "time for calc pc = 3.38 s\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.90      0.94      0.92     13675\n",
    "           1       0.65      0.49      0.56      2886\n",
    "\n",
    "   micro avg       0.86      0.86      0.86     16561\n",
    "   macro avg       0.77      0.72      0.74     16561\n",
    "weighted avg       0.85      0.86      0.86     16561\n",
    "\n",
    "------Embedding Method:freq weighted + SVD, DataSet:LCQMC---------\n",
    "time for calc matrix = 3068.15 s\n",
    "time for calc pc = 5.74 s\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.83      0.68      0.75      6250\n",
    "           1       0.73      0.86      0.79      6250\n",
    "\n",
    "   micro avg       0.77      0.77      0.77     12500\n",
    "   macro avg       0.78      0.77      0.77     12500\n",
    "weighted avg       0.78      0.77      0.77     12500\n",
    "\n",
    "------Embedding Method:freq weighted + SVD, DataSet:CCKS---------\n",
    "time for calc matrix = 1233.87 s\n",
    "time for calc pc = 2.07 s\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.81      0.84      0.82      5001\n",
    "           1       0.83      0.81      0.82      4999\n",
    "\n",
    "   micro avg       0.82      0.82      0.82     10000\n",
    "   macro avg       0.82      0.82      0.82     10000\n",
    "weighted avg       0.82      0.82      0.82     10000\n",
    "\n",
    "------Embedding Method:freq weighted + SVD, DataSet:ATEC---------\n",
    "time for calc matrix = 2236.74 s\n",
    "time for calc pc = 6.83 s\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.89      0.96      0.92     13675\n",
    "           1       0.68      0.45      0.54      2886\n",
    "\n",
    "   micro avg       0.87      0.87      0.87     16561\n",
    "   macro avg       0.78      0.70      0.73     16561\n",
    "weighted avg       0.85      0.87      0.86     16561"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
